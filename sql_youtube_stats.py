# -*- coding: utf-8 -*-
"""SQL Youtube Stats.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L8P23ieQodNfb-As5f0G4pkGRDCg_8bB

# Creation Tables
"""

import sqlite3
from pathlib import Path
import zipfile

if not Path("Resources/comments.csv").is_file():
    with zipfile.ZipFile("Resources/comments.zip", 'r') as zip_ref:
        zip_ref.extractall("Resources")
if not Path("Resources/videos_stats.csv").is_file():
    with zipfile.ZipFile("Resources/videos_stats.zip", 'r') as zip_ref:
        zip_ref.extractall("Resources")
if not Path("Resources/videos_stats.csv").is_file():
    with zipfile.ZipFile("Resources/videos_stats.zip", 'r') as zip_ref:
        zip_ref.extractall("Resources")
'Create DB youtube'

Path("youtube.db").touch()

'Connexion to DB youtube using SQLite'

conn = sqlite3.connect("youtube.db")
c = conn.cursor()

c.execute("drop table if exists stats")

c.execute("drop table if exists comments")

'Create empty table stats'

c.execute(
    """CREATE TABLE stats (
    Title text, `Video ID` text,
    `Published At` date, Keyword text, Likes int,
    Comments int, Views int
    );"""
)

"""Import CSV and create DataFrame 'stats'"""

import pandas as pd

# Open csv file
stats = pd.read_csv('Resources/videos_stats.csv', sep = ';')

from google.colab import drive
drive.mount('/content/drive')

import datetime as dt
stats['Published At'] = pd.to_datetime(stats['Published At'])

"""Create table 'comments'"""

c.execute(
    """CREATE TABLE comments (
    `Video ID` text,
    `Comment` text, Likes int,
    Sentiment int
    );"""
)

"""Import and create DataFrame 'comments'"""

comments = pd.read_csv("Resources/comments.csv", sep = ';')

stats.isna().sum()

comments.head()

comments.Sentiment.value_counts()

comments[(comments.Sentiment != '1.0') & (comments.Sentiment != '0.0') & (comments.Sentiment != '2.0')]

"""Drop missing values and not valid ones"""

stats.dropna(inplace=True)

comments.dropna(inplace=True)

comments = comments.drop(comments[(comments.Sentiment != '1.0') & (comments.Sentiment != '0.0') & (comments.Sentiment != '2.0')].index)

"""Fill both tables with datas from corresponding DataFrames"""

stats.to_sql("stats", conn, if_exists="append", index=False)
comments.to_sql("comments", conn, if_exists="append", index=False)

"""# Queries"""

com = """SELECT Title, SUM(Comments) AS Total_Comments 
FROM stats 
GROUP BY Title 
ORDER BY Total_Comments DESC 
LIMIT 10;
"""
top_com = pd.read_sql_query(com, conn)

"""Total amount of comments per Video Title, in descending order

"""

top_com

likes = """SELECT Title, SUM(Likes) AS Total_Likes 
FROM stats 
GROUP BY Title 
ORDER BY Total_Likes DESC 
LIMIT 10;
"""
top_likes = pd.read_sql_query(likes, conn)

"""Total amount of likes per Video Title, in descending order

"""

top_likes

'The most commented video is also the most liked one. And there is a ratio of 20 to 30 times more likes than comments for the most liked and commented videos.'

categories_views = """SELECT Keyword, SUM(Views) AS Total_Views
FROM stats 
GROUP BY Keyword 
ORDER BY Total_Views DESC
LIMIT 10;
"""
top_categories_views = pd.read_sql_query(categories_views, conn)

"""Total views per Category(Keyword), in descending order

"""

top_categories_views

categories_likes = """SELECT Keyword, SUM(Likes) AS Total_Likes
FROM stats 
GROUP BY Keyword 
ORDER BY Total_Likes DESC
LIMIT 10;
"""
top_categories_likes = pd.read_sql_query(categories_likes, conn)

"""Total likes per Category(Keyword), in descending order

"""

top_categories_likes

categories_comments = """SELECT Keyword, SUM(Comments) AS Total_Comments
FROM stats 
GROUP BY Keyword 
ORDER BY Total_Comments DESC
LIMIT 10;
"""
top_categories_comments = pd.read_sql_query(categories_comments, conn)

print('Total comments per Category(Keyword), in descending order')

top_categories_comments

"""As we can see in the queries the most liked category 'mrbeast', is also the most commented one. But it's not systematically the case for other categories. For example : history is the second most commented category but the 6th most liked one."""

com_likes = """SELECT Comment, SUM(Likes) AS Total_Likes
FROM comments 
GROUP BY Comment 
ORDER BY Total_Likes DESC
LIMIT 10;
"""
com_likes = pd.read_sql_query(com_likes, conn)

"""Total Likes per Comment (on video), in descending order

"""

com_likes

'The most liked comment is about subscribing to a channel after liking the video.'

ratio_per_video ="""SELECT stats.Title, 
       SUM(Views) AS total_views, 
       SUM(stats.likes) AS total_likes, 
       (SUM(Views)/SUM(stats.Likes)) AS view_like_ratio 
FROM stats 
JOIN comments ON stats.`Video ID` = comments.`Video ID` 
GROUP BY stats.Title
HAVING view_like_ratio > 0
ORDER BY view_like_ratio ASC;
"""
ratio_per_video = pd.read_sql_query(ratio_per_video, conn)

"""Total ratio of views and likes per video

"""

ratio_per_video

ratio_per_category ="""SELECT stats.Keyword, 
       SUM(Views) AS total_views, 
       SUM(stats.likes) AS total_likes, 
       (SUM(Views)/SUM(stats.Likes)) AS view_like_ratio 
FROM stats 
JOIN comments ON stats.`Video ID` = comments.`Video ID` 
GROUP BY Keyword
ORDER BY view_like_ratio ASC;

"""
ratio_per_category = pd.read_sql_query(ratio_per_category, conn)

print('Total ratio of views and likes per category')

ratio_per_category

'The category with the best ratio (so sum of comments close to sum of likes) is reaction follwed by gaming. It could mean that for both categories, the audience is quite loyal and active on related contents.'

avg_sentiment = """SELECT Keyword, AVG(Sentiment) AS avg_sentiment_score 
FROM comments 
JOIN stats ON stats.`Video ID` = comments.`Video ID` 
GROUP BY Keyword
ORDER BY avg_sentiment_score DESC;
"""
avg_sentiment = pd.read_sql_query(avg_sentiment, conn)

print('Average sentiment score per category')
avg_sentiment

'The category with the best sentiment score is lofi, followed by asmr. '

company = """SELECT Keyword, COUNT(*) AS company_apple_count 
FROM stats 
JOIN comments ON stats.`Video ID` = comments.`Video ID`
WHERE Comment LIKE '%Apple%'
GROUP BY Keyword 
ORDER BY company_apple_count DESC;
"""
company_apple = pd.read_sql_query(company, conn)

print('Occurency of company Apple in comments grouped by category')
company_apple

company = """SELECT Keyword, COUNT(*) AS company_samsung_count 
FROM stats 
JOIN comments ON stats.`Video ID` = comments.`Video ID`
WHERE Comment LIKE '%Samsung%'
GROUP BY Keyword 
ORDER BY company_samsung_count DESC;
"""
company_samsung = pd.read_sql_query(company, conn)

print('Occurency of company Apple in comments grouped by category')

company_samsung

"""The company Apple is more present in comments than samsung."""

stats.head()

comments.head()

year_views = """
    SELECT strftime('%Y', `Published At`) AS `Year`,
       SUM(Views) AS `Total Views`
FROM stats
GROUP BY `Year`
ORDER BY `Total Views` DESC ;

"""
year_views = pd.read_sql_query(year_views, conn)

print('Total Views per year')
year_views

year_likes = """
    SELECT strftime('%Y', `Published At`) AS `Year`,
       SUM(Likes) AS `Total Likes`
FROM stats
GROUP BY `Year`
ORDER BY `Total Likes` DESC;

"""
year_likes = pd.read_sql_query(year_likes, conn)

print('Total Likes per year')

year_likes

year_comments = """
    SELECT strftime('%Y', `Published At`) AS `Year`,
       SUM(Comments) AS `Total Comments`
FROM stats
GROUP BY `Year`
ORDER BY `Total Comments` DESC;

"""
year_comments = pd.read_sql_query(year_comments, conn)

print('Total Comments per year')
year_comments

"""2022 has been the year with the most comments and likes but not with the most views, as 2018 was the year where videos generated the biggest amount of views and with around 3 times less comments than 2022! 
It means that from 2018 to 2022, users were seeing less videos in general but were more engaged in participating and interacting with about the content. 
This could be explained by the fact that with the time, people were getting more used to the platform and were watching videos corresponding more to what they like. 
On top of it, the suggestion algorithm has been developped since then to provide more accurate and adapted videos to users.
"""

df = stats.merge(right = comments, on='Video ID', how = 'inner')

"""# Sentiment Analysis"""

comms = comments.Comment.str.cat()

test = 'I hope\ ok'

import re

def replace_backslash(chaine):
    r = re.compile(r"\\")
    result = r.sub(" ", chaine)
    return result
replace_backslash(test)

comms = replace_backslash(comms)

import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))

stop_words.update(['?', '!', '.', ',', ':', ';', '-', '--', '...', '"', "'", "they've", "they're", "they'll", "i've", "i'm", "i'll","could", "one", "get", "still","make","really",'since','would','video'])

from nltk.tokenize import TweetTokenizer
tokenizer = TweetTokenizer()
tokens = tokenizer.tokenize(comms)

print(len(tokens))
print(len(set(tokens)))

def stop_words_filtering(mots): 
    tokens = [mot for mot in mots if mot not in stop_words] 
    return tokens
tokens = stop_words_filtering(tokens)

print(len(tokens))

from PIL import Image
import numpy as np 
mask = np.array(Image.open('Resources/3399771-youtube-icon-editorial-vector-gratuit-vectoriel.jpg')) 
# Ins√©rez votre code ici
from wordcloud import WordCloud 
wc = WordCloud(background_color='white', max_words=1000, 
stopwords=stop_words, mask = np.array(Image.open("Resources/3399771-youtube-icon-editorial-vector-gratuit-vectoriel.jpg")), 
collocations = False, 
max_font_size=90, random_state=42) 
wc.generate(comms)

from wordcloud import ImageColorGenerator
import matplotlib.pyplot as plt
img_color = ImageColorGenerator(mask)
plt.figure(figsize= (30,20))
wc = wc.recolor(color_func=img_color)
wc.generate(comms)
plt.imshow(wc, interpolation="bilinear")
plt.axis("off")
plt.show()

sampled_comments = comments.sample(n=500)

sampled_comments.Sentiment = sampled_comments.Sentiment.astype(float)
sampled_comments['Sentiment'] = sampled_comments['Sentiment'].replace({0.0: 0, 1.0: 1, 2.0: 2})
sampled_comments['Sentiment'] = sampled_comments['Sentiment'].astype(int)



"""Machine Learning to classifiy by Sentiment using Neural Network (LSTM)

"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Prepare text data
texts = sampled_comments['Comment']  # a list of text samples
labels = sampled_comments['Sentiment']  # a list of corresponding labels



# Tokenize text data
max_words = 280
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

# Pad sequences to a fixed length
max_length = 50
data = pad_sequences(sequences, maxlen=max_length)

# Convert labels to categorical form
from tensorflow.keras.utils import to_categorical
categorical_labels = to_categorical(labels)

# Split the data into training and test sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data, categorical_labels, test_size=0.2)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

# Define the model
model = Sequential()
model.add(Embedding(max_words, 64, input_length=max_length))
model.add(LSTM(64))
model.add(Dense(3, activation='softmax'))

# Compile the model
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

# Define the model architecture
model = Sequential()
model.add(Embedding(max_words, 64, input_length=max_length))
model.add(LSTM(64, dropout=0.1))
model.add(Dense(3, activation='softmax'))

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Fit the model to the training data
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

# Evaluate the model on the test data
loss, accuracy = model.evaluate(X_test, y_test)
print('Test loss:', loss)
print('Test accuracy:', accuracy)

